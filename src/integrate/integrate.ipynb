{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integra√ß√£o\n",
    "\n",
    "* Verificar `parse.ipynb` antes para instru√ß√µes\n",
    "\n",
    "* Une datasets pelo crit√©rio de dist√¢ncia m√≠nima:\n",
    "    1. Para cada ponto de tempetura, obt√©m a sua data e encontra todos os pontos de $CO_2$ nessa mesma data;\n",
    "    2. Calcula a dist√¢ncia entre o ponto de temperatura e todos os outros pontos de $CO_2$ anteriormente selecionados;\n",
    "    3. Escolhe aquele de m√≠nima dist√¢ncia como o ponto equivalente entre os *datasets*.\n",
    "\n",
    "<br />\n",
    "\n",
    "* C√°lculo da dist√¢ncia:\n",
    "    * ‚úîÔ∏è **Abordagem 1:** plana ‚Äî $d(P_1, P_2) = \\sqrt{(x_1-x_2)^2 + (y_1-y_2)^2}$\n",
    "    * ‚ùì **Abordagem 2:** esf√©rica ‚Äî Faz sentido? √â necess√°ria? Como fazer?\n",
    "\n",
    "<br />\n",
    "\n",
    "* Sobre a implementa√ß√£o:\n",
    "    * N√£o encontrei uma forma direta e elegante para fazer a integra√ß√£o apenas atrav√©s da API do PySpark, ent√£o optei por apelar ao SQL üôè;\n",
    "    * A t√≠tulo de explora√ß√£o, foi feita uma integra√ß√£o utilizando `CROSS JOIN` e aplicando √† risca o m√©todo;\n",
    "        * Tal alternativa √© invi√°vel, haja vista que geraria registros numa ordem de grandeza de $10^{6 + 8}$.\n",
    "    * A t√≠tulo de usababilidade, foi feita a integra√ß√£o utilizando `LEFT JOIN` e `ROUND`, que possui o exato mesmo *result set* da alternativa precedente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas e Configura√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "spark = SparkSession.builder.config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"io.xskipper:xskipper-core_2.12:1.3.0\"\n",
    ").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xskipper import Xskipper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = \"./tmp/metadata\"\n",
    "\n",
    "config = dict([\n",
    "    (\"io.xskipper.parquet.mdlocation\", metadata_path),\n",
    "    (\"io.xskipper.parquet.mdlocation.type\", \"EXPLICIT_BASE_PATH_LOCATION\")\n",
    "])\n",
    "\n",
    "Xskipper.setConf(spark, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leitura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr_parsed_path = './data/tpr_data_parsed.csv'\n",
    "co2_parsed_path = './data/co2_data_parsed.csv'\n",
    "\n",
    "tpr_data = spark.read.options(header='True').format(\"csv\").load(tpr_parsed_path)\n",
    "co2_data = spark.read.options(header='True').format(\"csv\").load(co2_parsed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr_xskipper = Xskipper(spark, tpr_parsed_path)\n",
    "\n",
    "if tpr_xskipper.isIndexed(): tpr_xskipper.dropIndex()\n",
    "\n",
    "tpr_xskipper.indexBuilder()                   \\\n",
    "            .addMinMaxIndex(\"t_date\")         \\\n",
    "            .addValueListIndex(\"t_longitude\") \\\n",
    "            .addValueListIndex(\"t_latitude\")  \\\n",
    "            .build(tpr_data)\n",
    "\n",
    "\n",
    "co2_xskipper = Xskipper(spark, co2_parsed_path)\n",
    "\n",
    "if co2_xskipper.isIndexed(): co2_xskipper.dropIndex()\n",
    "\n",
    "co2_xskipper.indexBuilder()                   \\\n",
    "            .addValueListIndex(\"c_date\")         \\\n",
    "            .addValueListIndex(\"c_longitude\") \\\n",
    "            .addValueListIndex(\"c_latitude\")  \\\n",
    "            .build(co2_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Xskipper.isEnabled(spark): Xskipper.disable(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr_data.limit(int(1e3)).createOrReplaceTempView(\"tpr_data\")\n",
    "co2_data.limit(int(1e5)).createOrReplaceTempView(\"co2_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw query de integraliza√ß√£o\n",
    "\n",
    "cross_data = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT *\n",
    "    FROM (\n",
    "        -- Ordena√ß√£o crescente das dist√¢ncias,\n",
    "        -- orientado a data, latitude e longitude\n",
    "        SELECT\n",
    "            *,\n",
    "            ROW_NUMBER() OVER (\n",
    "                PARTITION BY\n",
    "                    t_date,\n",
    "                    t_latitude,\n",
    "                    t_longitude\n",
    "                ORDER BY distance ASC\n",
    "            ) AS row_num\n",
    "        FROM (\n",
    "            -- CROSS JOIN para c√°lculo de dist√¢ncias entre\n",
    "            -- todos os pontos dos datasets em uma mesma data\n",
    "            SELECT\n",
    "                *,\n",
    "                SQRT(POW(t_latitude - c_latitude, 2) + POW(t_longitude - c_longitude, 2)) AS distance\n",
    "            FROM tpr_data AS T\n",
    "            CROSS JOIN co2_data AS C\n",
    "            ON t_date = c_date\n",
    "        )\n",
    "    )\n",
    "    WHERE row_num = 1; -- Escolha dos registros com menor dist√¢ncia\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Xskipper.isEnabled(spark): Xskipper.enable(spark)\n",
    "\n",
    "tpr_data.limit(int(1e3)).createOrReplaceTempView(\"tpr_data\")\n",
    "co2_data.limit(int(1e5)).createOrReplaceTempView(\"co2_data\")\n",
    "\n",
    "# Raw query de integraliza√ß√£o\n",
    "\n",
    "cross_data = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT *\n",
    "    FROM tpr_data\n",
    "    LEFT JOIN co2_data\n",
    "    ON\n",
    "        t_date = c_date AND\n",
    "        t_latitude_rnd = c_latitude AND\n",
    "        t_longitude_rnd = c_longitude;\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "cross_data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Integraliza√ß√£o de fato\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Xskipper.isEnabled(spark): Xskipper.enable(spark)\n",
    "\n",
    "tpr_data.limit(1).createOrReplaceTempView(\"tpr_data\")\n",
    "co2_data.limit(int(1e5)).createOrReplaceTempView(\"co2_data\")\n",
    "\n",
    "# Raw query de integraliza√ß√£o\n",
    "\n",
    "cross_data = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT *\n",
    "    FROM tpr_data\n",
    "    LEFT JOIN co2_data\n",
    "    ON\n",
    "        t_date = c_date AND\n",
    "        t_latitude_rnd = c_latitude AND\n",
    "        t_longitude_rnd = c_longitude;\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "cross_data.show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07d19eb70bcfc2f0213fb2cc4f0e2b7647738b1a23b2bbd4e03e9ce97554aed1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
