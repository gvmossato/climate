{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integração\n",
    "\n",
    "* Requer:\n",
    "    * `./data/tpr_data_parsed.csv`;\n",
    "    * `./data/co2_data_parsed.csv`.\n",
    "* Saída em:\n",
    "    * `./out/final`.\n",
    "\n",
    "<br />\n",
    "\n",
    "* Verificar `parse.ipynb` antes para instruções;\n",
    "\n",
    "<br />\n",
    "\n",
    "* Une datasets pelo critério de distância mínima:\n",
    "    1. Para cada ponto de tempetura, obtém a sua data e encontra todos os pontos de $CO_2$ nessa mesma data;\n",
    "    2. Calcula a distância entre o ponto de temperatura e todos os outros pontos de $CO_2$ anteriormente selecionados;\n",
    "    3. Escolhe aquele de mínima distância como o ponto equivalente entre os *datasets*.\n",
    "\n",
    "<br />\n",
    "\n",
    "* Cálculo da distância:\n",
    "    * ✔️ **Abordagem 1:** plana — $d(P_1, P_2) = \\sqrt{(x_1-x_2)^2 + (y_1-y_2)^2}$;\n",
    "    * ❓ **Abordagem 2:** esférica — Faz sentido? É necessária? Como fazer?\n",
    "\n",
    "<br />\n",
    "\n",
    "* Sobre a implementação:\n",
    "    * Em um cenário ideial, a integração seria feita utilizando `CROSS JOIN` e aplicando à risca o método;\n",
    "        * Tal alternativa é inviável, haja vista que geraria registros numa ordem de grandeza de $10^{6 + 8}$.\n",
    "    * A título de usababilidade, foi feita a integração utilizando `LEFT JOIN` e adequação dos domínios dos valores de latitude e longitude.\n",
    "        * Em síntese, as coordenadas do *dataset* de temperatura passam a ser discretizadas tal como as presentes no de $CO_2$;\n",
    "        * Ambos os métodos possuem um exato mesmo *result set*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas e Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"io.xskipper:xskipper-core_2.12:1.3.0\"\n",
    ").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xskipper import Xskipper # Necessáriamente após build do spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = \"./tmp/metadata\"\n",
    "\n",
    "config = dict([\n",
    "    (\"io.xskipper.parquet.mdlocation\", metadata_path),\n",
    "    (\"io.xskipper.parquet.mdlocation.type\", \"EXPLICIT_BASE_PATH_LOCATION\")\n",
    "])\n",
    "\n",
    "Xskipper.setConf(spark, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leitura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr_parsed_path = './data/tpr_data_parsed.csv'\n",
    "co2_parsed_path = './data/co2_data_parsed.csv'\n",
    "\n",
    "tpr_reader = spark.read.options(header='True').format(\"csv\")\n",
    "co2_reader = spark.read.options(header='True').format(\"csv\")\n",
    "\n",
    "tpr_data = tpr_reader.load(tpr_parsed_path)\n",
    "co2_data = co2_reader.load(co2_parsed_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otimização\n",
    "\n",
    "* [Xskipper](https://xskipper.io/1.3.0/) é uma biblioteca que busca trazer uma indexação (alguma coisa) próxima a do SQL no Spark;\n",
    "* Em síntese, fornece opções para indexação de colunas baseada em critérios, por exemplo:\n",
    "    * Colunas com poucos dados únicos, mas muitos dados: fazer listagem dos valores únicos;\n",
    "    * Colunas com ordenação crescente/decrescente: obter valores limítrofes;\n",
    "    * Entre outras.\n",
    "* Todos esses metadados levantados a respeitos das colunas agilizam as operações (reduzem tempo de execução) pois permitem pular registros e registros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+\n",
      "|status |new_entries_added|old_entries_removed|\n",
      "+-------+-----------------+-------------------+\n",
      "|SUCCESS|1                |0                  |\n",
      "+-------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tpr_xskipper = Xskipper(spark, tpr_parsed_path)\n",
    "\n",
    "if tpr_xskipper.isIndexed(): tpr_xskipper.dropIndex()\n",
    "\n",
    "tpr_xskipper.indexBuilder()                 \\\n",
    "            .addValueListIndex(\"t_date\")    \\\n",
    "            .addValueListIndex(\"lon_grid\")  \\\n",
    "            .addValueListIndex(\"lat_grid\")  \\\n",
    "            .build(tpr_reader)              \\\n",
    "            .show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+\n",
      "|status |new_entries_added|old_entries_removed|\n",
      "+-------+-----------------+-------------------+\n",
      "|SUCCESS|1                |0                  |\n",
      "+-------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "co2_xskipper = Xskipper(spark, co2_parsed_path)\n",
    "\n",
    "if co2_xskipper.isIndexed(): co2_xskipper.dropIndex()\n",
    "\n",
    "co2_xskipper.indexBuilder()              \\\n",
    "            .addValueListIndex(\"c_date\") \\\n",
    "            .addValueListIndex(\"c_lon\")  \\\n",
    "            .addValueListIndex(\"c_lat\")  \\\n",
    "            .build(co2_reader)           \\\n",
    "            .show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Xskipper.isEnabled(spark): Xskipper.enable(spark)\n",
    "\n",
    "tpr_data.createOrReplaceTempView(\"tpr_data\")\n",
    "co2_data.createOrReplaceTempView(\"co2_data\")\n",
    "\n",
    "merged_data = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        t_date AS date,\n",
    "        t_lat AS lat,\n",
    "        t_lon AS lon,\n",
    "        city,\n",
    "        country,\n",
    "        tpr,\n",
    "        tpr_unc,\n",
    "        co2\n",
    "    FROM tpr_data\n",
    "    LEFT JOIN co2_data\n",
    "    ON\n",
    "        t_date = c_date AND\n",
    "        lon_grid = c_lon AND\n",
    "        lat_grid = c_lat\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A escrita dos dados leva até 6 minutos e resulta em, aproximadamente, 500 MB. Descomentar se necessário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_data.coalesce(1).write.format('csv').option('header', True).mode('overwrite').save('./out/final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07d19eb70bcfc2f0213fb2cc4f0e2b7647738b1a23b2bbd4e03e9ce97554aed1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
