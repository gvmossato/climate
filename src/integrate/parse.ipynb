{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas e Configurações\n",
    "\n",
    "1. Requer:\n",
    "    * `./data/GlobalLandTemperaturesByCity.csv`;\n",
    "    * `./data/co2_data.csv`.\n",
    "\n",
    "<br />\n",
    "\n",
    "2. É necessário possuir o **Java SE Development Kit 11** (JDK) instalado;\n",
    "    * Outras versões apresentam problemas;\n",
    "    * Possivel baixar o `jdk-11.0.16` através desse *[link](https://gist.github.com/wavezhang/ba8425f24a968ec9b2a8619d7c2d86a6)*.\n",
    "\n",
    "<br />\n",
    "\n",
    "3. Além disso, é necessário definir a variável de ambiente `JAVA_HOME` cujo valor é o diretório de instalação do JDK;\n",
    "    * Exemplo: `C:\\Progra~1\\Java\\jdk-11.0.16`;\n",
    "    * Em que `Progra~1` é, na verdade, a pasta `Program Files`.\n",
    "\n",
    "<br />\n",
    "\n",
    "4. Definir também as variáveis de ambiente:\n",
    "    * `PYSPARK_DRIVER_PYTHON` = `jupyter`;\n",
    "    * `PYSPARK_PYTHON` = `python`.\n",
    "\n",
    "<br />\n",
    "\n",
    "5. Além disso tudo, é necessário o Spark + Hadoop para salvar arquivos;\n",
    "    1. Assim sendo, efetue o download da versão 3.2.2 do [Spark](https://spark.apache.org/downloads.html);\n",
    "    2. Extraia os arquivos para um diretório desejado do computador (local de instalação);\n",
    "    3. Dentro desse diretório de instalação do Spark crie o diretório do Haddop: `<spark_path>\\hadoop\\bin`;\n",
    "    4. Efetuar o download da versão 3.3.1 do [Hadoop](https://github.com/kontext-tech/winutils)\n",
    "        * Aqui existe uma gambiarra:\n",
    "        1. Baixar o repositório como ZIP;\n",
    "        2. Extrair todos os arquivos da respectiva versão na pasta `bin` recém criada;\n",
    "        3. Dentro dessa pasta `bin`, criar uma nova pasta `bin` e repetir o passo anterior.\n",
    "\n",
    "<br />\n",
    "\n",
    "6. Definir as variáveis de ambiente:\n",
    "    * `SPARK_HOME`: diretório de instalação do Spark;\n",
    "    * `HADOOP_HOME` = `%SPARK_HOME%\\hadoop\\bin`.\n",
    "\n",
    "<br />\n",
    "\n",
    "7. Adicionar os caminhos dessa variáveis (`SPARK_HOME` E `HADOOP_HOME`) ao Path;\n",
    "\n",
    "<br />\n",
    "\n",
    "8. Será necessário reiniciar o computador após definir as variáveis de ambiente e alterar o Path.\n",
    "    * Há abaixo uma verificação para atestar se é possível ler todas as variáveis necessárias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "environ_vars = [\n",
    "    'JAVA_HOME',\n",
    "    'PYSPARK_DRIVER_PYTHON',\n",
    "    'PYSPARK_PYTHON',\n",
    "    'SPARK_HOME',\n",
    "    'HADOOP_HOME'\n",
    "]\n",
    "\n",
    "for var in environ_vars:\n",
    "    if not os.environ.get(var):\n",
    "        print(f'AVISO: Variável {var} não encontrada.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.utils import AnalysisException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leitura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:\npy4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist\r\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)\r\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)\r\n\tat py4j.Gateway.invoke(Gateway.java:237)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mgetOrCreate()\n\u001b[0;32m      3\u001b[0m tpr_data \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39moptions(header\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTrue\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mcsv(\u001b[39m'\u001b[39m\u001b[39m./data/GlobalLandTemperaturesByCity.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m co2_data \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39moptions(header\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTrue\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mcsv(\u001b[39m'\u001b[39m\u001b[39m./data/co2_data.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Files\\Poli\\PTC\\climate\\env\\lib\\site-packages\\pyspark\\sql\\session.py:272\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    269\u001b[0m     sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[0;32m    270\u001b[0m     \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    271\u001b[0m     \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[1;32m--> 272\u001b[0m     session \u001b[39m=\u001b[39m SparkSession(sc, options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_options)\n\u001b[0;32m    273\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     \u001b[39mgetattr\u001b[39m(\n\u001b[0;32m    275\u001b[0m         \u001b[39mgetattr\u001b[39m(session\u001b[39m.\u001b[39m_jvm, \u001b[39m\"\u001b[39m\u001b[39mSparkSession$\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mMODULE$\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    276\u001b[0m     )\u001b[39m.\u001b[39mapplyModifiableSettings(session\u001b[39m.\u001b[39m_jsparkSession, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n",
      "File \u001b[1;32md:\\Files\\Poli\\PTC\\climate\\env\\lib\\site-packages\\pyspark\\sql\\session.py:307\u001b[0m, in \u001b[0;36mSparkSession.__init__\u001b[1;34m(self, sparkContext, jsparkSession, options)\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[39mgetattr\u001b[39m(\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm, \u001b[39m\"\u001b[39m\u001b[39mSparkSession$\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mMODULE$\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mapplyModifiableSettings(\n\u001b[0;32m    304\u001b[0m             jsparkSession, options\n\u001b[0;32m    305\u001b[0m         )\n\u001b[0;32m    306\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 307\u001b[0m         jsparkSession \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mSparkSession(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsc\u001b[39m.\u001b[39;49msc(), options)\n\u001b[0;32m    308\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     \u001b[39mgetattr\u001b[39m(\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm, \u001b[39m\"\u001b[39m\u001b[39mSparkSession$\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mMODULE$\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mapplyModifiableSettings(\n\u001b[0;32m    310\u001b[0m         jsparkSession, options\n\u001b[0;32m    311\u001b[0m     )\n",
      "File \u001b[1;32md:\\Files\\Poli\\PTC\\climate\\env\\lib\\site-packages\\py4j\\java_gateway.py:1585\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1579\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1580\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_command_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1581\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1582\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1584\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1585\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1586\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gateway_client, \u001b[39mNone\u001b[39;49;00m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fqn)\n\u001b[0;32m   1588\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1589\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32md:\\Files\\Poli\\PTC\\climate\\env\\lib\\site-packages\\py4j\\protocol.py:330\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 330\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n\u001b[0;32m    333\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    334\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    335\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    336\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name))\n",
      "\u001b[1;31mPy4JError\u001b[0m: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:\npy4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist\r\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)\r\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)\r\n\tat py4j.Gateway.invoke(Gateway.java:237)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "tpr_data = spark.read.options(header='True').csv('./data/GlobalLandTemperaturesByCity.csv')\n",
    "co2_data = spark.read.options(header='True').csv('./data/co2_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|             6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-02-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-03-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tpr_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Dataset* de temperaturas:\n",
    "\n",
    "* `dt` (string): data da amostra;\n",
    "* `AverageTemperature` (float): temperatura média medida;\n",
    "* `AverageTemperatureUncertainty` (float): incerteza associada a temperatura medida;\n",
    "* `City` (string): cidade de origem da amostra;\n",
    "* `Country` (string): país de origem da amostra;\n",
    "* `Latitude` (string): graus de latitude norte da amostra;\n",
    "* `Longitude` (string):  graus de longitude leste da amostra;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+-----------------+\n",
      "|     Times|LatDim|LonDim|            value|\n",
      "+----------+------+------+-----------------+\n",
      "|1850-01-01|     0|     0|288.1340637207031|\n",
      "|1850-01-01|     0|     1|288.1340637207031|\n",
      "|1850-01-01|     0|     2|288.1340637207031|\n",
      "|1850-01-01|     0|     3|288.1340637207031|\n",
      "|1850-01-01|     0|     4|288.1340637207031|\n",
      "+----------+------+------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "co2_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Dataset* de temperaturas:\n",
    "\n",
    "* `Times` (string): data da amostra;\n",
    "* `LatDim` (int): graus de latitude norte da amostra;\n",
    "* `LonDim` (int): graus de longitude leste da amostra;\n",
    "* `value` (float): concentração de $CO_2$ em ppm;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tratamento\n",
    "\n",
    "* Define uma mesma data de início para ambos os datasets\n",
    "* Realiza correção de tipos de dados e padroniza colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padroniza datasets iniciando em 1850-01-01\n",
    "tpr_data = tpr_data[tpr_data['dt'] >= '1850-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padroniza nomes de colunas entre datasets\n",
    "\n",
    "trp_columns_rename = [\n",
    "    ('dt', 't_date'),\n",
    "    ('AverageTemperature', 't_temperature'),\n",
    "    ('AverageTemperatureUncertainty', 't_temperature_unc'),\n",
    "    ('City', 't_city'),\n",
    "    ('Country', 't_country'),\n",
    "    ('Latitude', 't_latitude'),\n",
    "    ('Longitude', 't_longitude'),\n",
    "]\n",
    "\n",
    "co2_columns_rename = [\n",
    "    ('Times', 'c_date'),\n",
    "    ('LatDim', 'c_latitude'),\n",
    "    ('LonDim', 'c_longitude'),\n",
    "    ('value', 'c_co2'),\n",
    "]\n",
    "\n",
    "for old, new in trp_columns_rename:\n",
    "    tpr_data = tpr_data.withColumnRenamed(old, new)\n",
    "\n",
    "for old, new in co2_columns_rename:\n",
    "    co2_data = co2_data.withColumnRenamed(old, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retira 'N' e 'E' do final dos valores de \n",
    "# latitude/longitude e converte para float\n",
    "\n",
    "to_strip = ['t_latitude', 't_longitude']\n",
    "\n",
    "for col in to_strip:\n",
    "    tpr_data = tpr_data.withColumn(\n",
    "        col,\n",
    "        expr(f\"float(substring({col}, 1, length({col})-1))\")\n",
    "    )\n",
    "\n",
    "    tpr_data = tpr_data.withColumn(\n",
    "        col + '_rnd',\n",
    "        expr(f\"int(round({col}))\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[t_date: string, t_temperature: string, t_temperature_unc: string, t_city: string, t_country: string, t_latitude: float, t_longitude: float, t_latitude_rnd: int, t_longitude_rnd: int]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpr_data.withColumn(\n",
    "        col,\n",
    "        expr(f\"float(substring({col}, 1, length({col})-1))\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------------+------+---------+----------+-----------+--------------+---------------+\n",
      "|    t_date|       t_temperature|t_temperature_unc|t_city|t_country|t_latitude|t_longitude|t_latitude_rnd|t_longitude_rnd|\n",
      "+----------+--------------------+-----------------+------+---------+----------+-----------+--------------+---------------+\n",
      "|1850-01-01|              -5.265|             1.82| Århus|  Denmark|     57.05|      10.33|            57|             10|\n",
      "|1850-02-01|               1.859|            1.641| Århus|  Denmark|     57.05|      10.33|            57|             10|\n",
      "|1850-03-01|0.031999999999999806|            3.167| Århus|  Denmark|     57.05|      10.33|            57|             10|\n",
      "|1850-04-01|  5.7639999999999985|            1.903| Århus|  Denmark|     57.05|      10.33|            57|             10|\n",
      "|1850-05-01|              11.037|            0.586| Århus|  Denmark|     57.05|      10.33|            57|             10|\n",
      "+----------+--------------------+-----------------+------+---------+----------+-----------+--------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tpr_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+-----------------+\n",
      "|    c_date|c_latitude|c_longitude|            c_co2|\n",
      "+----------+----------+-----------+-----------------+\n",
      "|1850-01-01|         0|          0|288.1340637207031|\n",
      "|1850-01-01|         0|          1|288.1340637207031|\n",
      "|1850-01-01|         0|          2|288.1340637207031|\n",
      "|1850-01-01|         0|          3|288.1340637207031|\n",
      "|1850-01-01|         0|          4|288.1340637207031|\n",
      "+----------+----------+-----------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "co2_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Escrita dos datasets parseados, comentar se não desejado. Somente um por vez, haja vista que os arquivos são sobrescritos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tpr_data.coalesce(1).write.format('csv').option('header', True).mode('overwrite').save('./out/parsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# co2_data.coalesce(1).write.format('csv').option('header', True).mode('overwrite').save('./out/parsed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07d19eb70bcfc2f0213fb2cc4f0e2b7647738b1a23b2bbd4e03e9ce97554aed1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
